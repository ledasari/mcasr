Mismatched transcripts are produced by non-speakers of a language, writing down what they hear, as if it were nonsense syllables in their own language.  Let's say they only speak English.  Let's call the utterance language L2.

There are actually two distinct sources of variability: English orthography, and L2-to-English misperception.  I think we should solve these in two separate ways.  English orthography should be solved by, first, aligning the mismatched transcripts to the audio using an English-language (nonsense) speech recognizer.  _After_ that problem is solved, L2-to-English misperception should be solved by generating candidate L2 transcriptions (word transcriptions!!!), using them to train an L2 recognizer, then aligning them to the audio.
 
Software components necessary to implement this idea:
 
(i)                  ENGLISH NONSENSE DICTIONARY:  A pronunciation lexicon, lexiconp.txt in kaldiâ€™s terminology, of English nonsense words.  Fortunately, this only needs to cover exactly the nonsense words used by Turkers in their transcripts --- the vocabulary should be exactly the set of space-bounded nonsense words used in the mismatched transcripts.  The necessary code, then, should take each of those words, and look in http://www.isle.illinois.edu/sst/data/g2ps/English/English_ref_orthography_dict.html to find every possible pronunciation of that nonsense word.    Every possible pronunciation of that nonsense word should be listed as a separate pronunciation candidate in lexiconp.txt.  The units of English_ref_orthography_dict.txt are designed so that, if a digraph or trigraph exists, you should prefer the digraph or trigraph instead of using the single-grapheme.  We can implement that preference by assigning a probability to each pronunciation proportional to exp(-(number of entries from English_ref_orthography.html that were concatenated in order to form this candidate pronunciation)).

(ii)                ENGLISH ASR AND FORCED ALIGNMENT:  An English-language GMM-HMM ASR, trained using the above lexiconp.txt, i.e., using only the set of words that were actually used by Turkers.  Treat every Turker transcription as an independent training token, so if we have three-fold redundancy, then the amount of training data is 3X the amount of audio.   Forced alignment using (i) and (ii) will choose the most likely pronunciation of each Turker nonsense word, maximizing p(audio|pronunciation). 

(iii)               L2 PRONUNCIATION DICTIONARY IN TERMS OF ENGLISH PHONES: Take all available monolingual texts in the foreign language.  In LORELEI, we have about 120k words of text in each foreign language, so we have a very large set of words in that language.  Generate a lexiconp.txt, a lexicon with pronunciation probabilities, using this list of words, combined with http://www.isle.illinois.edu/sst/data/g2ps/ reference orthography in the foreign language, combined with some kind of mapping from the L2 phonemes to English phonemes.  The result should be a dictionary in which each line contains an L2 word, followed by a probability, followed by a list of English phonemes.

(iv)               MINIMUM-STRING-EDIT GENERATION OF CANDIDATE L2 WORD TRANSCRIPTIONS: Calculate the ten L2 word sequences that best match the English-language phone transcripts generated by forced alignment from each Turker transcript (thus, thirty word sequences per audio clip).

(v)                 L2 PRONUNCIATION DICTIONARY IN TERMS OF L2 PHONES: Back off from (iii) to a dictionary using the L2 phone set, i.e., no more English phones.

(vi)               TRAIN AN L2 ASR, USE IT TO DO FORCED ALIGNMENT OF THE L2 WORD TRANSCRIPTIONS.
