## Mismatched Crowdsourcing Automatic Speech Recognition
### Train an ASR directly from mismatched transcripts, based on [kaldi/egs/librispeech](https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech).

<!-- https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet -->

Mismatched transcripts are produced by people writing down what they
hear in a language L2, as if it were nonsense syllables in their own
language, English.

This has two sources of variability, which we solve separately:

- To solve **English orthography**, we align a transcript to its audio with a nonsense-English ASR.

- To solve **L2-to-English misperception**, we generate candidate L2 word transcriptions,
use them to train an L2 recognizer, and then align them to the audio.
 
We need these software components:

### 0. Submit audio to Mechanical Turk.

[Instructions and scripts.](./0-mturk/)

### 1. English nonsense dictionary

Make a pronunciation lexicon of English nonsense words, which Kaldi calls `lexiconp.txt`.
Its vocabulary is the space-delimited words in Turker transcripts.
Find each word's pronunciations with www.isle.illinois.edu/sst/data/g2ps/English/English_ref_orthography_dict.html, and list them in `lexiconp.txt`.
This dictionary's units are designed so that you should prefer digraphs and trigraphs to single graphemes.
To implement that, to each candidate pronunciation assign a probability proportional to exp(-(number of concatenated dictionary entries that form this pronunciation)).

Scripts are in the subfolder [1-nonsenseDict](./1-nonsenseDict).
The script [1-nonsenseDict/split-words.rb](1-nonsenseDict/split-words.rb) will also preprocess turker transcripts (like <https://github.com/uiuc-sst/PTgen/blob/master/steps/preprocess_turker_transcripts.pl>).

### 2. English ASR and forced alignment

An English-language GMM-HMM ASR, trained using (1)'s `lexiconp.txt`,
and the G2P's www.isle.illinois.edu/sst/data/g2ps/English/ISLEdict.html (for context-sensitive English mappings) and
www.isle.illinois.edu/sst/data/g2ps/all_orthography_dicts.txt (for all other languages).
Because ISLEdict is much larger, the combined G2P will prefer English pronunciations when available.
Treat every Turker transcription as an independent training token.
So if we have 3 transcriptions per utterance, then the amount of training data is 3 times the amount of audio.
Forced alignment then chooses each nonsense word's likeliest pronunciation, maximizing p(audio|pronunciation). 

### 3. L2 pronunciation dictionary with English phones

Collect all available monolingual texts in L2.  LORELEI should give us 120k words.  Using
*(i)* this list of words,
*(ii)* the L2 reference orthography from www.isle.illinois.edu/sst/data/g2ps/, and
*(iii)* some kind of mapping from L2 phonemes to English phonemes,
generate another `lexiconp.txt`,
whose lines each contain an L2 word, a list of English phonemes (a pronunciation), and that pronunciation's probability.

### 4. Minimum-string-edit generation of candidate L2 word transcriptions

Calculate the 10 L2-word sequences that best match the English-phone transcripts
generated by forced alignment from each transcript (thus, 3 * 10 = 30 word sequences per utterance).

### 5. L2 pronunciation dictionary with L2 phones

Convert (3)'s `lexiconp.txt`, which uses English phones, to one that uses L2 phones.

### 6. Train an L2 ASR.

Use the ASR to do forced alignment of the L2 word transcriptions from (5).
